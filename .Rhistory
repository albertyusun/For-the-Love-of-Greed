knitr::opts_chunk$set(echo = TRUE)
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
library(dplyr)
library(tidytext)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
short <- read.csv("CSVs/metadata-short.csv")
knitr::opts_chunk$set(echo = TRUE)
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text_df <- tibble(1:4, text = text)
text_df %>%
unnest_tokens(word, text)
austen_books
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words
book_words <- left_join(book_words, total_words)
book_words
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
slice(1:9) %>%
unnest_tokens(word, booktext) %>%
count(title,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(title) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~text, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~text, ncol = 2, scales = "free_y")
TCP_words
View(TCP_words)
austen_books
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
ggplot(book_words, aes(n/total, fill = book)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
slice(1:9) %>%
unnest_tokens(word, booktext) %>%
count(title,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(title) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
TCP_words
austen_books
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
ggplot(book_words, aes(n/total, fill = book)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~book, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = text)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~text, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = title)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~title, ncol = 2, scales = "free_y")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
short <- read.csv("CSVs/metadata-short.csv")
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
slice(1:9) %>%
unnest_tokens(word, booktext) %>%
count(author,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(author) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
custom_stop <- tibble(word = c("a","and","the","of","to"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
tidy_TCP %>%
count(word, sort = TRUE)
ggplot(TCP_words, aes(n/total, fill = title)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~title, ncol = 2, scales = "free_y")
ggplot(TCP_words, aes(n/total, fill = author)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~author, ncol = 2, scales = "free_y")
install.packages("topicmodels")
knitr::opts_chunk$set(echo = TRUE)
short <- read.csv("CSVs/metadata-short.csv")
View(short)
View(short)
View(short)
short <- read.csv("CSVs/metadata-shorter.csv")
short <- read.csv("CSVs/metadata-shorter.csv")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
short <- read.csv("CSVs/metadata-shorter.csv")
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
slice(1:9) %>%
unnest_tokens(word, booktext) %>%
count(author,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(author) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
custom_stop <- tibble(word = c("a","and","the","of","to","Page Â [unnumbered]"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is", "it", "be", "but", "for", "you", "not", "in", "Page Â [unnumbered]"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
data("AssociatedPress")
data("AssociatedPress")
AssociatedPress
View(AssociatedPress)
short
## Use VectorSource before using Corpus
myCorpus <- Corpus(VectorSource(short$booktext))
tdm <- TermDocumentMatrix(myCorpus)
inspect(tdm)
short <- read.csv("CSVs/metadata-shorter.csv", fileEncoding="latin1")
short <- read.csv("CSVs/metadata-shorter.csv", header=TRUE, stringsAsFactors = FALSE, fileEncoding="latin1")
short <- read.csv("CSVs/metadata-shorter.csv", header=TRUE, stringsAsFactors = FALSE, fileEncoding="latin1")
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(tidyverse)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(tidy)
short <- read.csv("CSVs/metadata-shorter.csv", header=TRUE, stringsAsFactors = FALSE, encoding="UTF-8")
short <- read.csv("CSVs/metadata-shorter.csv", encoding="UTF-8")
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is", "it", "be", "but", "for", "you", "not", "in", "Page Â [unnumbered]"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
View(short)
data("AssociatedPress")
AssociatedPress
## Use VectorSource before using Corpus
myCorpus <- Corpus(VectorSource(short$booktext))
tdm <- TermDocumentMatrix(myCorpus)
inspect(tdm)
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is", "it", "be", "but", "for", "you", "not", "in", "[unnumbered]"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is",
"it", "be", "but", "for", "you", "not", "in",
"[unnumbered]"))
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(custom_stop)
tidy_TCP
data("AssociatedPress")
AssociatedPress
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is",
"it", "be", "but", "for", "you", "not", "in",
"[unnumbered]","ã","thou","thy","hense","\210â", "doth", "1", "2",
"â","3","page","unnumbered"))
data(stop_words)
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(stop_words) %>%
anti_join((custom_stop))
tidy_TCP
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
short <- read.csv("CSVs/metadata-shorter.csv", encoding="UTF-8")
#encoding removes the weird A symbols
#short <- read.csv("C:\\Users\\andre\\Metadata\\MetaThird2\\metaThird2.csv")
#short <- fread('C:\\Users\\andre\\Metadata\\MetaThird2\\metaThird2.csv')
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
unnest_tokens(word, booktext) %>%
count(author,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(author) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is",
"it", "be", "but", "for", "you", "not", "in",
"[unnumbered]","ã","thou","thy","hense","\210â", "doth", "1", "2",
"â","3","page","unnumbered"))
data(stop_words)
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(stop_words) %>%
anti_join((custom_stop))
tidy_TCP
tidy_TCP %>%
count(word, sort = TRUE)
ggplot(TCP_words, aes(n/total, fill = author)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~author, ncol = 2, scales = "free_y")
data("AssociatedPress")
AssociatedPress
## Use VectorSource before using Corpus
myCorpus <- Corpus(VectorSource(short$booktext))
tdm <- TermDocumentMatrix(myCorpus)
inspect(tdm)
## Use VectorSource before using Corpus
short_Corpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
short_lda <- LDA(short_ldm, k = 2, control = list(seed = 1234))
short_lda <- LDA(short_tdm, k = 2, control = list(seed = 1234))
short_lda
short_lda <- LDA(short_tdm, k = 2, control = list(seed = 1234))
short_lda
short_topics <-(short_lda, matrix = "beta")
short_topics <- tidy(short_lda, matrix = "beta")
library(reshape2)
install.packages("reshape2")
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
library(plyr)
library(plyr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
short_topics <- tidy(short_lda, matrix = "beta")
short_topics
top_terms <- short_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
knitr::opts_chunk$set(echo = TRUE)
top_terms <- short_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
short <- read.csv("CSVs/metadata-shorter.csv", encoding="UTF-8")
#encoding removes the weird A symbols
#short <- read.csv("C:\\Users\\andre\\Metadata\\MetaThird2\\metaThird2.csv")
#short <- fread('C:\\Users\\andre\\Metadata\\MetaThird2\\metaThird2.csv')
#use the first 9 texts for now because the last one is super long
TCP_words <- short %>%
unnest_tokens(word, booktext) %>%
count(author,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(author) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is",
"it", "be", "but", "for", "you", "not", "in",
"[unnumbered]","ã","thou","thy","hense","\210â", "doth", "1", "2",
"â","3","page","unnumbered"))
data(stop_words)
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(stop_words) %>%
anti_join((custom_stop))
tidy_TCP
tidy_TCP %>%
count(word, sort = TRUE)
ggplot(TCP_words, aes(n/total, fill = author)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~author, ncol = 2, scales = "free_y")
## Use VectorSource before using Corpus
short_Corpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
short_lda <- LDA(short_tdm, k = 2, control = list(seed = 1234))
short_lda
short_topics <- tidy(short_lda, matrix = "beta")
short_topics
library(stm)
install.packages("stm")
install.packages("quanteda")
library(stm)
library(quanteda)
