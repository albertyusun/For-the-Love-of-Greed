#load packages
library(plyr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
setwd("~/Desktop/Git/For-the-Love-of-Greed2")
#load packages
library(plyr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
#load data Encoding: move weird A symbols
metadata <- read.csv("CSVs/1690-1700.csv",stringsAsFactors=FALSE, fileEncoding="latin1")
TCP_words <- metadata %>%
unnest_tokens(word, booktext) %>%
dplyr:: count(author,word,sort=TRUE)
#load data Encoding: move weird A symbols
metadata <- read.csv("CSVs/1690-1700.csv",stringsAsFactors=FALSE, fileEncoding="latin1")
TCP_words <- metadata %>%
unnest_tokens(word, booktext) %>%
dplyr:: count(author,word,sort=TRUE)
#load data Encoding: move weird A symbols
short <- read.csv("CSVs/metadata-shorter.csv", encoding="UTF-8")
TCP_words <- short %>%
unnest_tokens(word, booktext) %>%
count(author,word,sort=TRUE)
#creates list of total amount of words in a func
total_words <- TCP_words %>%
group_by(author) %>%
summarize(total = sum(n))
TCP_words <- left_join(TCP_words, total_words)
TCP_words
custom_stop <- tibble(word = c("a","and","the","of","to","that", "in", "is",
"it", "be", "but", "for", "you", "not", "in",
"[unnumbered]","ã","thou","thy","hense","\210â", "doth", "1", "2",
"â","3","page","unnumbered"))
data(stop_words)
#data(custom_stop)
tidy_TCP <- TCP_words %>%
anti_join(stop_words) %>%
anti_join((custom_stop))
tidy_TCP
short_tdm
short_Corpus
tidy_TCP %>%
count(word, sort = TRUE)
short_Corpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
install.packages("tmap")
short_Corpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
knitr::opts_chunk$set(echo = TRUE)
myCorpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
myCorpus <- Corpus(VectorSource(short$booktext))
short_tdm <- TermDocumentMatrix(myCorpus)
inspect(short_tdm)
tidy_skipgrams <- tidy_TCP %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
cleantext <- tidy_TCP[:,2]
tidy_TCP %>%
select(booktext)
tidy_TCP %>%
select(word)
words <- tidy_TCP %>%
select(word)
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
words <- tidy_TCP %>%
select(c('word'))
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, postID, ngramID) %>%
unnest_tokens(word, ngram)
wordID <- row.names(words)
#create context window with length 8
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, ngramID) %>%
unnest_tokens(word, ngram)
unigram_probs <- words %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
words
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text, n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, ngramID) %>%
unnest_tokens(word, ngram)
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))}
skipgram_probs <- tidy_skipgrams %>%
pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
mutate(p = n / sum(n))
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, ngramID) %>%
unnest_tokens(word, ngram)
unnest_tokens(words, ngram)
unnest_tokens(word, ngrams)
unnest_tokens('words', ngram)
install.packages("widyr")
#load packages
library(plyr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(tm)
library(topicmodels)
library(data.table)
library(reshape2)
library(widyr)
yes
tidy_skipgrams <- words %>%
unnest_tokens(ngram, text, token = "ngrams", n = 8) %>%
mutate(ngramID = row_number()) %>%
tidyr::unite(skipgramID, ngramID) %>%
unnest_tokens(word, ngram)
words
strsplit(words, " ")[[1]]
strsplit(words, "\\s+")[[1]]
library(vctrs)
vec_is(words)
unigram_probs <- words %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
mutate(p = n / sum(n))
